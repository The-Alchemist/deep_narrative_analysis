{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17bd74dc",
   "metadata": {},
   "source": [
    "# Exploration of spaCy and NLP\n",
    "\n",
    "* Using previous sentences for coreference and time/place \n",
    "* Time ... during, before/after, on anniversary of, after EVENT, ...\n",
    "* Adverbial clause modifying noun\n",
    "* Causal ... when, because, because of, caused, caused by, as a result, resulted, resulted in, affected, since, due to, had effect of, therefore, so,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc0b292c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-25T15:16:24.594230Z",
     "start_time": "2021-06-25T15:16:21.275440Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x13dcf9dc0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports and set up\n",
    "import copy\n",
    "import spacy\n",
    "from spacy.matcher import DependencyMatcher\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "\n",
    "nlp = spacy.load('en_core_web_trf')\n",
    "nlp.add_pipe('sentencizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae0d40b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T03:30:03.983645Z",
     "start_time": "2021-06-26T03:30:03.347100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Words that introduce a 'causal' clause, where the main clause is the effect\n",
    "cause_connectors = ['when', 'because', 'since', 'as']  \n",
    "# Prepositions that introduce a cause in the form of a noun phrase\n",
    "cause_prepositions = ['because of', 'due to', 'as a result [of]', 'as a consequence [of]']\n",
    "# Words that introduce a 'causal' effect in the main clause, where other clause is the cause\n",
    "effect_connectors = ['so', 'therefore ', 'consequently ']\n",
    "# If - then only is cause-effect when the tenses of the main and other clause are the same\n",
    "# TODO\n",
    "cause_effect_pairs = [('if', 'then')]\n",
    "\n",
    "text = \"In 1940, the Soviet Union occupied Bukovina. A year later, when Romania joined Nazi Germany \"\\\n",
    "       \"in the war against the Soviet Union, the Soviets were driven from Stanesti. Mobs then carried out \"\\\n",
    "       \"bloody attacks on the town's Jews. During the violence, I and my family fled to Czernowitz \"\\\n",
    "       \"with the aid of the local police chief. In fall of 1941, my family were forced to settle in the \"\\\n",
    "       \"Czernowitz ghetto, where living conditions were poor and they were subject to deportation \"\\\n",
    "       \"to Transnistria. In 1943, I and Beatrice escaped from the ghetto using false papers that their \"\\\n",
    "       \"father had obtained. After escaping to the Soviet Union, I and Beatrice returned to Czechoslovakia \"\\\n",
    "       \"after World War II, where they were eventually reunited with their parents. \"\\\n",
    "       \"My family was together, so I was happy.\"\n",
    "nlp_text = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eacf7f64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T03:35:01.507852Z",
     "start_time": "2021-06-26T03:35:01.485946Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# From nlp.py \n",
    "def check_subject_in_clause(cls_sentence: Span) -> (list, list):\n",
    "    # Returns a list of the verb and connector tokens\n",
    "    clausal_verbs = []\n",
    "    connectors = []\n",
    "    # First check adverbial clauses, which will have the advmod in the advcl\n",
    "    clausal_verbs = [child for child in cls_sentence.root.children if child.dep_ in ('advcl')]\n",
    "    if len(clausal_verbs) > 0:\n",
    "        connectors = [conn for conn in clausal_verbs[0].children if conn.dep_ in ('advmod', 'mark')]\n",
    "    else:\n",
    "        connectors = []\n",
    "    # Also check for clausal complement, which may have the advmod in the original clause\n",
    "    new_verbs = [child for child in cls_sentence.root.children if child.dep_ in ('ccomp')]\n",
    "    if new_verbs:\n",
    "        clausal_verbs.extend(new_verbs)\n",
    "        new_connectors = [conn for conn in new_verbs[0].children if conn.dep_ in ('advmod', 'mark')]\n",
    "        if not new_connectors:\n",
    "            new_connectors = [conn for conn in cls_sentence.root.children if conn.dep_ in ('advmod', 'mark')]\n",
    "        if new_connectors:\n",
    "            connectors.extend(new_connectors)\n",
    "    return clausal_verbs, connectors\n",
    "    \n",
    "    \n",
    "    \n",
    "def remove_startswith(chunk: str, connector: str) -> str:\n",
    "    # Returns string removing second string from start\n",
    "    if chunk.startswith(f'{connector} '):\n",
    "        chunk = chunk[len(connector) + 1:]\n",
    "    elif chunk.startswith(f' {connector} '):\n",
    "        chunk = chunk[len(connector) + 2:]\n",
    "    return chunk\n",
    "\n",
    "\n",
    "def get_chunks(verb: Token, connector: list, chunk_sentence: Span, is_conj: bool) -> list:\n",
    "    # Return list of strings that are the chunks\n",
    "    chunks = []\n",
    "    # Is there a subject of the other clause's verb? That is required to split the sentence\n",
    "    # An 'expl' is the word, 'there'\n",
    "    subj2 = [subj for subj in verb.children if ('subj' in subj.dep_ or subj.dep_ == 'expl')]\n",
    "    seen = set()\n",
    "    if len(subj2):\n",
    "        # Yes ... Separate clauses\n",
    "        # Get the tokens in sentence related to the 'other' verb's subtree\n",
    "        seen = [ww for ww in verb.subtree if ww.pos_ != 'PUNCT']\n",
    "        seen_chunk = ' '.join([ww.text for ww in seen]).strip() + '.'\n",
    "        unseen = [ww for ww in chunk_sentence if ww not in seen and ww.pos_ != 'PUNCT']\n",
    "        unseen_chunk = ' '.join([ww.text for ww in unseen]).strip() + '.'\n",
    "        seen_first = False\n",
    "        for conn in connector: \n",
    "            if is_conj:\n",
    "                # Remove the connector words (to prevent cycles and sentences such as 'Mary biked to the market and')\n",
    "                unseen_chunk = unseen_chunk.replace(f' {conn.text} ', ' ').replace(f' {conn.text}.', '.')\n",
    "            else: \n",
    "                # Connector in the seen words and should seen words be first because they are a \n",
    "                # cause related to an effect?\n",
    "                if not seen_first and conn.text in seen_chunk and conn.text in cause_connectors:\n",
    "                    seen_first = True\n",
    "                # Remove the connector words (to prevent cycles) but NOT if there is a single auxiliary verb \n",
    "                # If so, then the advmod is needed\n",
    "                # For example, 'my daughter is home' (is = auxiliary verb, home = adverbial modifier)\n",
    "                if not any([ww for ww in seen if ww.pos_ == 'AUX']) or any([ww for ww in seen if ww.pos_ == 'VERB']):\n",
    "                    seen_chunk = remove_startswith(seen_chunk, conn.text) \n",
    "                    seen_chunk = seen_chunk.replace(f' {conn.text} ', ' ').replace(f' {conn.text}.', '.')\n",
    "                if not any([ww for ww in unseen if ww.pos_ == 'AUX']) or any([ww for ww in unseen if ww.pos_ == 'VERB']):\n",
    "                    unseen_chunk = remove_startswith(unseen_chunk, conn.text)   \n",
    "        # Store the seen and unseen clauses as separate sentences\n",
    "        if seen_first:\n",
    "            chunks.append(seen_chunk)\n",
    "            chunks.append(unseen_chunk)\n",
    "        else:\n",
    "            chunks.append(unseen_chunk)\n",
    "            chunks.append(seen_chunk)\n",
    "    if len(chunks) > 0:\n",
    "        return chunks\n",
    "    else:\n",
    "        return [chunk_sentence.text]\n",
    "    \n",
    "\n",
    "def split_by_conjunctions(conj_sentence: Span) -> (list, str):\n",
    "    # Return list of sentences (strings) and a string that is the connector (if any)\n",
    "    conj_sents = []\n",
    "    conj_verb = [child for child in conj_sentence.root.children if child.dep_ == 'conj']\n",
    "    connectors = [conn for conn in conj_sentence.root.children if conn.dep_ == 'cc']\n",
    "    if len(conj_verb) > 0:\n",
    "        for chunk in get_chunks(conj_verb[0], connectors, conj_sentence, True):\n",
    "            conj_sents.append(str(chunk))\n",
    "    else:\n",
    "        conj_sents = [conj_sentence.text]\n",
    "    return conj_sents, ('' if  not connectors else connectors[0].text)\n",
    "    \n",
    "    \n",
    "def split_clauses(sentence: Span, ce_dict: dict) -> list:\n",
    "    # Return list of Spans\n",
    "    orig_sents = [sentence.text]\n",
    "    # Iterate until the sentences cannot be further decomposed/split\n",
    "    while True:\n",
    "        new_sents = []\n",
    "        # Go through each sentence in the array\n",
    "        for orig_sent in orig_sents:\n",
    "            intermed_sents = []\n",
    "            # Semi-colons automatically split sentences\n",
    "            if '; ' in orig_sent:\n",
    "                semicolon_index = orig_sent.index('; ')\n",
    "                new_sents = [f'{orig_sent[:semicolon_index]}.', f'{orig_sent[semicolon_index + 2:]}.']\n",
    "                break\n",
    "            # First split by words such as 'and', 'but', 'or', ... related to the 'root' verb\n",
    "            # TODO: Deal with 'or', 'nor' as the splitting word below\n",
    "            for chunk_sent in nlp(orig_sent).sents:\n",
    "                # nlp(orig_sent) 're-tokenizes' in order to create a new Document\n",
    "                # This allows .sents to get individual sentences, since sentences have 'root' verbs \n",
    "                intermed_sents, splitting_word = split_by_conjunctions(chunk_sent)\n",
    "                # Now check resulting sentences to further split by adverbial clauses and clausal complements\n",
    "                # Only split if there is a subject for a related verb\n",
    "                # Example: 'When I went to the store, I met George.' ('when ...' is an adverbial modifier in the clause)\n",
    "                for intermed_sent in intermed_sents:\n",
    "                    for chunk_sent2 in nlp(intermed_sent).sents:\n",
    "                        # nlp(intermed_sent) 're-tokenizes' in order to create a new Document, as above\n",
    "                        clausal_verbs, connectors = check_subject_in_clause(chunk_sent2)\n",
    "                        # Is this a cause-effect connector?\n",
    "                        found_cause = any([conn for conn in connectors if conn.text.lower() in cause_connectors])\n",
    "                        found_effect = any([ww for ww in chunk_sent2 if ww.text.lower() in effect_connectors])\n",
    "                        # Need to have both a clause and a connector/modifier for this logic\n",
    "                        if len(clausal_verbs) > 0:\n",
    "                            chunks = get_chunks(clausal_verbs[0], connectors, chunk_sent2, False)\n",
    "                            if found_cause:\n",
    "                                # Save cause-effect\n",
    "                                ce_dict[chunks[0]] = chunks[1]\n",
    "                                new_sents.append(chunks[1])\n",
    "                            elif found_effect:\n",
    "                                # Save cause-effect\n",
    "                                ce_dict[chunks[1]] = chunks[0]\n",
    "                                new_sents.append(chunks[0])\n",
    "                            else:\n",
    "                                new_sents.extend(chunks)\n",
    "                        else:\n",
    "                            new_sents.append(intermed_sent)\n",
    "                \n",
    "        # Check if the processing has resulted in new sentence clauses\n",
    "        if len(orig_sents) == len(new_sents):\n",
    "            # If not, break out of the while loop\n",
    "            break\n",
    "        else:\n",
    "            # New clauses, so keep processing\n",
    "            orig_sents = copy.deepcopy(new_sents)\n",
    "    return new_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c60b301b-3fc4-4d1b-8a3e-8a70ee27dd5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T03:35:05.995044Z",
     "start_time": "2021-06-26T03:35:03.591368Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp_sentences = []\n",
    "cause_effect_dict = dict()\n",
    "for sentence in nlp_text.sents:\n",
    "    # Sentence splitting\n",
    "    for sent in split_clauses(sentence, cause_effect_dict):\n",
    "        sent_nlp = nlp(sent)\n",
    "        # Determine the spans of individual nouns and noun chunks\n",
    "        spans = list(sent_nlp.ents) + list(sent_nlp.noun_chunks)  \n",
    "        spans = spacy.util.filter_spans(spans)\n",
    "        # Reset the sentence parse to maintain chunks\n",
    "        with sent_nlp.retokenize() as retokenizer:\n",
    "            [retokenizer.merge(span, attrs={'tag': span.root.tag,\n",
    "                                            'dep': span.root.dep}) for span in spans]\n",
    "        # Store new sentence details\n",
    "        nlp_sentences.append(sent_nlp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d81dffc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T03:35:07.160956Z",
     "start_time": "2021-06-26T03:35:07.157645Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[In 1940, the Soviet Union occupied Bukovina., A year later the Soviets were driven from Stanesti., Mobs then carried out bloody attacks on the town's Jews., During the violence, I and my family fled to Czernowitz with the aid of the local police chief., In fall of 1941, my family were forced to settle in the Czernowitz ghetto, where living conditions were poor and they were subject to deportation to Transnistria., In 1943, I and Beatrice escaped from the ghetto using false papers that their father had obtained., After escaping to the Soviet Union I and Beatrice returned to Czechoslovakia after World War II., they were reunited with their parents., so I was happy.]\n",
      "\n",
      "{'Romania joined Nazi Germany in the war against the Soviet Union.': 'A year later the Soviets were driven from Stanesti.', 'My mother was home.': 'so I was happy.'}\n"
     ]
    }
   ],
   "source": [
    "print(nlp_sentences)\n",
    "print()\n",
    "print(cause_effect_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144px",
    "left": "994px",
    "right": "20px",
    "top": "121px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
