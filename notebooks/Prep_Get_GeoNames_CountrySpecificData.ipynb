{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Purpose:\n",
    "  * Retrieve the feature details for select countries from the **GeoNames** download site, http://download.geonames.org/export/dump\n",
    "    * The countries of interest are defined by a comma-separated list of their ISO-2 alpha codes\n",
    "      * The list is specified in the dna.config parameter, countries\n",
    "    * Each country's data is captured in GeoNames download site's XX.zip files (where XX is the country's 2 character ISO code)\n",
    "    * The zip files are decompressed by the code below, and the XX.txt file is parsed for the data\n",
    "  * Capture all the feature-level detail from the countries:\n",
    "    * GeoNames id (at tab 0)\n",
    "    * ASCII name and alternate names (at tabs 2 and 3)\n",
    "    * Latitude and longitude (at tabs 4 and 5)\n",
    "    * Feature code and feature class (at tabs 6 and 7; for example, feature code P indicates a city, village, etc. while the feature class PPLF indicates a farm village)\n",
    "      * Full details on the feature classes and codes can be found at https://www.geonames.org/export/codes.html\n",
    "      * Also, these details are captured in an ontology file, geonames_featureCodes_X.ttl (where X represents the feature class code)\n",
    "    * \"Containing\" countries (at tabs 8 and 9)\n",
    "    * Elevation (at tab 15)\n",
    "  * Retrieve the containment hierarchy for the entities using the REST API, api.geonames.org/hierarchy?geonameId=xxxx&username=uuuu\n",
    "  * Encode all the above data as triples and save to two files:\n",
    "    * geonames_XX.ttl with all the location entities for the country identified by XX\n",
    "    * geonames_containment_XX.ttl with the containment details for the locations (if any are defined)\n",
    "    \n",
    "After the files are fully encoded, they are loaded to Stardog and moved to the directory, ../Ontologies/country_data.\n",
    "\n",
    "Before running this program, make sure that:\n",
    "  * Stardog is started (via the command, stardog-admin server start) \n",
    "  * The dna.config file (used in the second executing cell) has been updated for your environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T21:23:29.174279Z",
     "start_time": "2020-02-19T21:23:27.665465Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "import time\n",
    "from datetime import date\n",
    "import zipfile, shutil\n",
    "import configparser as cp\n",
    "import pickle\n",
    "import xml.etree.ElementTree as ET\n",
    "import requests, wget\n",
    "import stardog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T21:23:31.530599Z",
     "start_time": "2020-02-19T21:23:31.527120Z"
    }
   },
   "outputs": [],
   "source": [
    "commaStr = ','\n",
    "dotZipStr = '.zip'\n",
    "dotTTLStr = '.ttl'\n",
    "geonamesContainmentStr = 'geonames_containment_'\n",
    "geonamesStr = 'geonames_'\n",
    "pluralStr = ' (plural)'\n",
    "\n",
    "stardogConfig = 'StardogConfig'\n",
    "\n",
    "today = date.today().strftime(\"%B %d, %Y\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T21:23:36.482985Z",
     "start_time": "2020-02-19T21:23:36.452066Z"
    }
   },
   "outputs": [],
   "source": [
    "mappingDict = {}\n",
    "with open('featureCodes_mappings.pickle', 'rb') as mdHandle:\n",
    "    mappingDict = pickle.load(mdHandle)\n",
    "\n",
    "countryDict = {}\n",
    "with open('continent_country_geonames.pickle', 'rb') as cdHandle:\n",
    "    countryDict = pickle.load(cdHandle)\n",
    "\n",
    "countryCodeDict = {}\n",
    "with open('country_isoCodes_and_names.pickle', 'rb') as ccHandle:\n",
    "    countryCodeDict = pickle.load(ccHandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get access details from dna.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T21:23:41.093030Z",
     "start_time": "2020-02-19T21:23:41.076225Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get access details from the dna.config file, stored in the same directory as the .ipynb file\n",
    "config = cp.RawConfigParser()\n",
    "config.read('dna.config')\n",
    "        \n",
    "# Set Stardog connection details\n",
    "sdConnDetails = {\n",
    "    'endpoint': config.get(stardogConfig, 'endpoint'),\n",
    "    'username': config.get(stardogConfig, 'username'),\n",
    "    'password': config.get(stardogConfig, 'password')\n",
    "}\n",
    "\n",
    "# Set Stardog database name\n",
    "dbName = config.get(stardogConfig, 'dbName')\n",
    "\n",
    "# Set path to directory where ontologies stored\n",
    "ontolPath = config.get('OntologiesConfig', 'ontolPath')\n",
    "if not ontolPath.endswith('/'):\n",
    "    ontolPath = f'{ontolPath}/'\n",
    "\n",
    "# Get GeoNames user id\n",
    "geonamesUser = config.get('GeoNamesConfig', 'geonamesUser')\n",
    "\n",
    "# Get the countries of interest\n",
    "countries = config.get('CountriesOfInterest','countries')\n",
    "countries = [country.strip() for country in countries.split(',')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the feature data from the country-specific zips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T21:23:50.081854Z",
     "start_time": "2020-02-19T21:23:48.877375Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Get the features .zip files\n",
    "for country in countries:\n",
    "    # Does the file already exist in the directory? If so, don't repeat\n",
    "    if os.path.exists(f'{ontolPath}country_data/{geonamesStr}{country}.ttl'):\n",
    "        break;\n",
    "    # File needs to be downloaded\n",
    "    with open(f'{geonamesStr}{country}{dotTTLStr}', 'w') as ttlFile:\n",
    "        # Write the prefix details\n",
    "        ttlFile.write('@prefix : <urn:ontoinsights:ontology:dna:> .\\n'\\\n",
    "                      '@prefix dna: <urn:ontoinsights:ontology:dna:> .\\n'\\\n",
    "                      '@prefix geo: <urn:ontoinsights:ontology:geonames:> .\\n'\\\n",
    "                      '@prefix owl: <http://www.w3.org/2002/07/owl#> .\\n'\\\n",
    "                      '@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\\n'\\\n",
    "                      '@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\\n'\\\n",
    "                      '@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\\n\\n'\\\n",
    "                      '########################################################################.\\n'\\\n",
    "                      '# File defining location data for ')\n",
    "        ttlFile.write(f'{country} ({countryCodeDict[country]})\\n')\n",
    "        ttlFile.write('# \\n') \n",
    "        ttlFile.write(f'# Created: {today}\\n')\n",
    "        ttlFile.write(f'# Last modified: {today}\\n')\n",
    "        ttlFile.write('########################################################################\\n\\n')\n",
    "        # Get the features .zip file for the country\n",
    "        wget.download(f'http://download.geonames.org/export/dump/{country}{dotZipStr}')\n",
    "        # Unpack the zip and get the contents of the XX.txt file\n",
    "        with zipfile.ZipFile(f'{country}{dotZipStr}', 'r') as countryZip:\n",
    "            countryData = countryZip.read(f'{country}.txt').decode('utf-8')\n",
    "        # Remove the zip file \n",
    "        os.remove(f'{country}{dotZipStr}')\n",
    "        \n",
    "        # Parse the data in countriesData\n",
    "        countryLines = countryData.split('\\n')\n",
    "        for locLine in countryLines:  \n",
    "            locDetails = [ld.strip() for ld in locLine.split('\\t')]    # Data is separated by tabs\n",
    "            if len(locDetails) > 8:                                    # Ignore lines w/out sufficient tabs\n",
    "                geoId = locDetails[0]\n",
    "                # Determine the type of the location (based on its feature class and code)\n",
    "                fCode = f'{locDetails[6]}_{locDetails[7]}'\n",
    "                if fCode in mappingDict:\n",
    "                    fCode = mappingDict[fCode]\n",
    "                plural = False\n",
    "                if pluralStr in fCode:\n",
    "                    plural = True\n",
    "                    fCode = fCode.replace(pluralStr, '')\n",
    "                # Get the names of the location \n",
    "                label = locDetails[2]               # ASCII name\n",
    "                nonAsciiName = locDetails[1]\n",
    "                # Create Turtle for the country's data, with the GeoNames ID as the local part of the entity's IRI\n",
    "                ttlFile.write(f'geo:{geoId} rdf:type :{fCode} ; \\n  rdfs:label \"{label}\"@en ; \\n')\n",
    "                ttlFile.write(f'  :latitude \"{locDetails[4]}\"^^xsd:decimal ; \\n  :longitude \"{locDetails[5]}\"^^xsd:decimal ')\n",
    "                synNames = ''\n",
    "                if label != nonAsciiName:           # Include the non-ASCII location name in synonyms\n",
    "                    synNames = nonAsciiName\n",
    "                if locDetails[3]:\n",
    "                    if synNames:\n",
    "                        synNames += ',' + locDetails[3]\n",
    "                    else:\n",
    "                        synNames = locDetails[3]\n",
    "                if synNames:\n",
    "                    synNames = synNames.replace(label, '').replace(nonAsciiName, '')    # Get rid of duplicates\n",
    "                    # Clean up the text\n",
    "                    synNames = synNames.replace(',,', ',')                             \n",
    "                    if synNames.startswith(','):\n",
    "                        synNames = synNames[1:]                           \n",
    "                    if synNames.endswith(','):\n",
    "                        synNames = synNames[:-1]\n",
    "                    # Make sure that there still is some text and then format as individual strings\n",
    "                    if synNames:\n",
    "                        synNames = synNames.replace(',', '\", \"')\n",
    "                        ttlFile.write(f'; \\n  :synonym \"{synNames}\" ')\n",
    "                # Write 'plural True' if applicable\n",
    "                if plural:\n",
    "                    ttlFile.write('; \\n  :plural true ')\n",
    "                # Write elevation details if available\n",
    "                if locDetails[15]:      \n",
    "                    ttlFile.write(f'; \\n  :altitude_meters \"{locDetails[15]}\"^^xsd:decimal .\\n')\n",
    "                else:\n",
    "                    ttlFile.write('. \\n')\n",
    "                # Indicate that the location is part of a country, if defined\n",
    "                containingCountrySet = set()\n",
    "                if locDetails[8]:\n",
    "                    containingCountrySet.add(locDetails[8])\n",
    "                if locDetails[9]:\n",
    "                    containingCountries = locDetails[9].split(',')\n",
    "                    for containing in containingCountries:\n",
    "                        containingCountrySet.add(containing)\n",
    "                for cc in containingCountrySet:\n",
    "                    if cc in countryDict:\n",
    "                        ttlFile.write(f'geo:{countryDict[cc]} :has_component geo:{geoId} . \\n')\n",
    "                # Finished processing the location, add a new line\n",
    "                ttlFile.write('\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the place hierarchy details\n",
    "\n",
    "Note that this execution is LONG-RUNNING due to need to pace requests to the GeoNames server. For example, getting the containment data for the Ethiopian locations (extracted from ET.zip) resulted in almost 20000 locations (approx 20-21  hours elapsed time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T00:55:24.643765Z",
     "start_time": "2020-02-16T16:07:20.871272Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GeoNames requests: 7685\n"
     ]
    }
   ],
   "source": [
    "# Create a set to hold the unique container-contained pairs (to avoid duplication of data in the TTL file)\n",
    "containingSet = set()\n",
    "count = 0\n",
    "\n",
    "# Get the new locations that were encoded above\n",
    "for country in countries:\n",
    "    # Set up the output file\n",
    "    with open(f'geonames_containment_{country}.ttl', 'w') as outTTLFile:\n",
    "            outTTLFile.write('@prefix : <urn:ontoinsights:ontology:dna:> .\\n'\\\n",
    "                             '@prefix dna: <urn:ontoinsights:ontology:dna:> .\\n'\\\n",
    "                             '@prefix geo: <urn:ontoinsights:ontology:geonames:> .\\n\\n'\\\n",
    "                             '########################################################################\\n'\\\n",
    "                             '# File defining containment data for locations in ')\n",
    "            outTTLFile.write(f'{country} ({countryCodeDict[country]})\\n')\n",
    "            outTTLFile.write('# \\n')\n",
    "            outTTLFile.write(f'# Created: {today}\\n')\n",
    "            outTTLFile.write(f'# Last modified: {today}\\n')\n",
    "            outTTLFile.write('########################################################################\\n\\n')\n",
    "    # Process through the country-specific GeoNames file to retrieve the IDs\n",
    "    lineNumber = 0\n",
    "    found = False\n",
    "    with open(f'geonames_{country}.ttl', 'r') as inTTLFile:\n",
    "        for line in inTTLFile:\n",
    "            lineNumber += 1\n",
    "            if not ' rdf:type ' in line:\n",
    "                continue                # Only care about retrieving the geoIds, found in the string, geo:XXXX rdf:type loc:YYYY\n",
    "            geoId = line.split()[0][4:]  \n",
    "            if geoId == '339927':\n",
    "                found = True\n",
    "            if not(found):\n",
    "                continue\n",
    "            try:\n",
    "                response = requests.get('http://api.geonames.org/hierarchy?geonameId={}&username={}'.\n",
    "                                        format(geoId, geonamesUser))\n",
    "            except:                     # In-Process details when error occurred\n",
    "                print(f'Country: {country}')\n",
    "                print(f'Count: {count}')\n",
    "                print(f'Line number: {lineNumber}')\n",
    "                print(f'GeoId: {geoId}')\n",
    "            count += 1\n",
    "            root = ET.fromstring(response.content)\n",
    "            containing = []\n",
    "            for child in root:\n",
    "                if child.tag == 'geoname':\n",
    "                    # Skip the obvious containers (Earth, the continent and the country)\n",
    "                    if child[0].text == 'Earth' or child[8].text == 'CONT' or child[8].text == 'PCLI':\n",
    "                        continue               \n",
    "                    containing.append(child[4].text)\n",
    "            \n",
    "            # If not > 1 containing entities, the location is either the country itself\n",
    "            #   or is contained by the country (which is already known from above)\n",
    "            if len(containing) > 1:               \n",
    "                for i in range(1, len(containing)):\n",
    "                    if not (containing[i-1], containing[i]) in containingSet:\n",
    "                        containingSet.add((containing[i-1], containing[i]))\n",
    "                        with open(f'{geonamesContainmentStr}{country}{dotTTLStr}', 'a') as containingTTLFile:\n",
    "                            containingTTLFile.write(\n",
    "                                f'geo:{containing[i-1]} :has_component geo:{containing[i]} . \\n')\n",
    "           \n",
    "            # Pace requests to geonames server (< 1000/hr)\n",
    "            time.sleep(3.75)            # Pace requests to geonames server (< 1000/hr)                \n",
    "            # Pace requests to geonames server (< 20000/day)\n",
    "            if count > 19900:              # Assuming 1 request every 3.75 seconds = 20.73 hrs to 19900 requests\n",
    "                time.sleep(12600)          # Sleeping 3.5 hours resets the 24hr clock\n",
    "                count = 0\n",
    "\n",
    "# Print count details\n",
    "print(f'Total GeoNames requests: {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store the data in the Stardog repository and move the country files \n",
    "\n",
    "The files are moved to the directory specified by the ontolPath parameter in the dna.config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for country in countries:\n",
    "    # Open connection to Stardog\n",
    "    conn = stardog.Connection(dbName, **sdConnDetails)\n",
    "    # Load ontologies\n",
    "    conn.begin()\n",
    "    conn.add(stardog.content.File(f'{geonamesStr}{country}{dotTTLStr}'))\n",
    "    conn.add(stardog.content.File(f'{geonamesContainmentStr}{country}{dotTTLStr}'))\n",
    "    conn.commit()\n",
    "    # Move files\n",
    "    shutil.move(f'{geonamesStr}{country}{dotTTLStr}', \n",
    "                f'{ontolPath}country_data/{geonamesStr}{country}{dotTTLStr}')\n",
    "    shutil.move(f'{geonamesContainmentStr}{country}{dotTTLStr}', \n",
    "                f'{ontolPath}country_data/{geonamesContainmentStr}{country}{dotTTLStr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
